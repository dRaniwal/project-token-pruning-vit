{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b09694a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-08T10:51:01.386809Z",
     "iopub.status.busy": "2025-12-08T10:51:01.386596Z",
     "iopub.status.idle": "2025-12-08T10:51:09.875606Z",
     "shell.execute_reply": "2025-12-08T10:51:09.874694Z"
    },
    "papermill": {
     "duration": 8.493342,
     "end_time": "2025-12-08T10:51:09.876880",
     "exception": false,
     "start_time": "2025-12-08T10:51:01.383538",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs: 2\n"
     ]
    }
   ],
   "source": [
    "# Pruning ViT model, Starts Pruning after 4th epoch.",
    "# Basic imports and device setup\n",
    "import os, time, math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Use GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"GPUs:\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29075cfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T10:51:09.881847Z",
     "iopub.status.busy": "2025-12-08T10:51:09.881543Z",
     "iopub.status.idle": "2025-12-08T10:51:09.885232Z",
     "shell.execute_reply": "2025-12-08T10:51:09.884706Z"
    },
    "papermill": {
     "duration": 0.007221,
     "end_time": "2025-12-08T10:51:09.886244",
     "exception": false,
     "start_time": "2025-12-08T10:51:09.879023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Hyperparameters / configuration\n",
    "# -----------------------------\n",
    "\n",
    "# Vision Transformer (ViT) architecture\n",
    "IMG = 48          # input image resolution (CIFAR10 32x32 -> resized to 48x48)\n",
    "PATCH = 4         # patch size (each patch is 4x4 pixels)\n",
    "DIM = 512         # embedding dimension\n",
    "DEPTH = 8         # number of transformer blocks\n",
    "HEADS = 8         # number of attention heads\n",
    "NUM_CLASSES = 10  # CIFAR10\n",
    "\n",
    "# Training schedule\n",
    "EPOCHS = 10\n",
    "WARMUP_EPOCHS = 4  # number of epochs without pruning (full tokens)\n",
    "\n",
    "# Token pruning configuration\n",
    "R_MAX = 0.6        # maximum fraction of tokens to drop at the deepest layer\n",
    "ALPHA = 2.0        # controls how drop rate increases with depth\n",
    "MIN_TOKENS = 8     # minimum number of non-CLS tokens to keep per layer\n",
    "\n",
    "# Optimization\n",
    "LR = 3e-4\n",
    "BS = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bdccffd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T10:51:09.890637Z",
     "iopub.status.busy": "2025-12-08T10:51:09.890441Z",
     "iopub.status.idle": "2025-12-08T10:51:09.895608Z",
     "shell.execute_reply": "2025-12-08T10:51:09.895075Z"
    },
    "papermill": {
     "duration": 0.008633,
     "end_time": "2025-12-08T10:51:09.896644",
     "exception": false,
     "start_time": "2025-12-08T10:51:09.888011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"Multi-head self-attention module.\n",
    "\n",
    "    Returns both the projected output and the attention weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, heads=8):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        # Scale factor for dot-product attention (1/sqrt(d_k))\n",
    "        self.scale = (dim // heads) ** -0.5\n",
    "        # Single linear layer to generate query, key, value (Q, K, V)\n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "        # Final projection after concatenating heads\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"x: [B, N, C] where\n",
    "        B = batch size, N = number of tokens, C = embedding dim\n",
    "        \"\"\"\n",
    "        B, N, C = x.shape\n",
    "        H = self.heads\n",
    "\n",
    "        # Compute Q, K, V and reshape to [B, N, 3, H, C//H]\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, H, C // H)\n",
    "        # Split the 3-tuple dimension into separate tensors: [B, N, H, C//H]\n",
    "        q, k, v = qkv.unbind(2)\n",
    "\n",
    "        # Rearrange to [B, H, N, C//H] for batched attention computation\n",
    "        q = q.permute(0, 3, 1, 2)\n",
    "        k = k.permute(0, 3, 1, 2)\n",
    "        v = v.permute(0, 3, 1, 2)\n",
    "\n",
    "        # Scaled dot-product attention: [B, H, N, N]\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(-1)\n",
    "\n",
    "        # Apply attention to values and merge heads back to [B, N, C]\n",
    "        out = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        return self.proj(out), attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "939028f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T10:51:09.901044Z",
     "iopub.status.busy": "2025-12-08T10:51:09.900850Z",
     "iopub.status.idle": "2025-12-08T10:51:09.905988Z",
     "shell.execute_reply": "2025-12-08T10:51:09.905509Z"
    },
    "papermill": {
     "duration": 0.00859,
     "end_time": "2025-12-08T10:51:09.906942",
     "exception": false,
     "start_time": "2025-12-08T10:51:09.898352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Feed-forward network used inside each transformer block.\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, dim * 4)\n",
    "        self.fc2 = nn.Linear(dim * 4, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # GELU activation is standard for transformers\n",
    "        return self.fc2(F.gelu(self.fc1(x)))\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Single transformer encoder block: LN -> MHA -> residual -> LN -> MLP -> residual.\"\"\"\n",
    "    def __init__(self, dim, heads):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = Attention(dim, heads)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp  = MLP(dim)\n",
    "\n",
    "    def forward(self, x, return_attn=False):\n",
    "        # Multi-head attention with pre-norm\n",
    "        a, attn = self.attn(self.norm1(x))\n",
    "        x = x + a\n",
    "        # Feed-forward network with pre-norm\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        if return_attn:\n",
    "            # Optionally return attention map for analysis / pruning\n",
    "            return x, attn\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d452307",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T10:51:09.911240Z",
     "iopub.status.busy": "2025-12-08T10:51:09.911047Z",
     "iopub.status.idle": "2025-12-08T10:51:09.916486Z",
     "shell.execute_reply": "2025-12-08T10:51:09.915930Z"
    },
    "papermill": {
     "duration": 0.008795,
     "end_time": "2025-12-08T10:51:09.917485",
     "exception": false,
     "start_time": "2025-12-08T10:51:09.908690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    \"\"\"Baseline Vision Transformer for CIFAR-10.\n",
    "\n",
    "    This version does not perform token pruning – it operates on all tokens.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Patch embedding: 3xHxW -> DIM x (H/PATCH) x (W/PATCH)\n",
    "        self.patch_embed = nn.Conv2d(3, DIM, PATCH, PATCH)\n",
    "        N = (IMG // PATCH) ** 2  # number of patches (tokens) per image\n",
    "\n",
    "        # CLS token and positional embeddings (CLS + N patches)\n",
    "        self.cls = nn.Parameter(torch.zeros(1, 1, DIM))\n",
    "        self.pos = nn.Parameter(torch.zeros(1, 1 + N, DIM))\n",
    "\n",
    "        # Transformer encoder blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(DIM, HEADS) for _ in range(DEPTH)\n",
    "        ])\n",
    "\n",
    "        # Final layer norm and classification head\n",
    "        self.norm = nn.LayerNorm(DIM)\n",
    "        self.head = nn.Linear(DIM, NUM_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "\n",
    "        # Patch embedding: [B, 3, H, W] -> [B, N, DIM]\n",
    "        x = self.patch_embed(x).flatten(2).transpose(1, 2)\n",
    "\n",
    "        # Prepend CLS token\n",
    "        x = torch.cat([self.cls.expand(B, -1, -1), x], dim=1)\n",
    "\n",
    "        # Add positional embeddings (truncate if tokens were fewer)\n",
    "        x = x + self.pos[:, :x.size(1), :]\n",
    "\n",
    "        # Pass through all transformer blocks\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        # CLS-based classification\n",
    "        x = self.norm(x)\n",
    "        return self.head(x[:, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84541b90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T10:51:09.921594Z",
     "iopub.status.busy": "2025-12-08T10:51:09.921386Z",
     "iopub.status.idle": "2025-12-08T10:51:09.928576Z",
     "shell.execute_reply": "2025-12-08T10:51:09.928010Z"
    },
    "papermill": {
     "duration": 0.010494,
     "end_time": "2025-12-08T10:51:09.929616",
     "exception": false,
     "start_time": "2025-12-08T10:51:09.919122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimplePrunedViT(nn.Module):\n",
    "    \"\"\"Wrapper around a ViT that performs simple, training-time token pruning.\n",
    "\n",
    "    At each layer after warmup, we:\n",
    "      1. Compute attention on the full set of tokens.\n",
    "      2. Score tokens by how much other tokens attends to them based on the attention weights.\n",
    "      3. Keep only the top-K tokens (plus CLS) and drop the rest.\n",
    "      4. Run attention + MLP on the reduced token set.\n",
    "    \"\"\"\n",
    "    def __init__(self, vit_model):\n",
    "        super().__init__()\n",
    "        self.m = vit_model\n",
    "        self.L = len(vit_model.blocks)  # number of layers\n",
    "\n",
    "    def forward(self, x, epoch=None):\n",
    "        B = x.size(0)\n",
    "\n",
    "        # --- Patch embedding and positional encodings ---\n",
    "        # [B, 3, H, W] -> [B, N, DIM]\n",
    "        x = self.m.patch_embed(x).flatten(2).transpose(1, 2)\n",
    "        cls = self.m.cls.expand(B, -1, -1)\n",
    "\n",
    "        # Concatenate CLS token and add positional encodings\n", 
    "        x = torch.cat([cls, x], dim=1)\n",
    "        x = x + self.m.pos[:, :x.size(1), :]\n",
    "\n",
    "        # --- Layer-wise pruning ---\n",
    "        for l, blk in enumerate(self.m.blocks):\n",
    "            # No pruning during warmup or when epoch is not provided\n",
    "            if epoch is None or epoch <= WARMUP_EPOCHS:\n",
    "                x = blk(x)\n",
    "                continue\n",
    "\n",
    "            # 1. Compute attention on current tokens (without changing x yet)\n",
    "            x_norm = blk.norm1(x)\n",
    "            _, attn = blk.attn(x_norm)  # attn: [B, H, N, N]\n",
    "\n",
    "            # Number of non-CLS tokens currently present\n",
    "            N = x.size(1) - 1\n",
    "            if N <= 1:\n",
    "                # Not enough tokens to prune; fall back to standard block\n",
    "                x = blk(x)\n",
    "                continue\n",
    "\n",
    "            # 2. Compute token importance score from CLS attention\n",
    "            #    - mean over heads, pick CLS row, drop CLS column, then mean over batch\n",
    "            #    Result shape: [N]\n",
    "            score = attn.mean(1)[:, 0, 1:1 + N].mean(0)\n",
    "\n",
    "            # 3. Decide how many tokens to keep\n",
    "            #    Drop ratio increases with layer depth up to R_MAX.\n",
    "            drop = R_MAX * ((l + 1) / self.L) ** ALPHA\n",
    "            keep = int(N * (1 - drop))\n",
    "\n",
    "            # Ensure we keep at least 1 token and at most N\n",
    "            keep = max(1, min(N, keep))\n",
    "            # Additionally enforce global minimum token count\n",
    "            keep = max(MIN_TOKENS, keep)\n",
    "\n",
    "            # 4. Select top-K tokens according to scores (excluding CLS)\n",
    "            _, idx = torch.topk(score, keep)  # indices in [0, N-1] for non-CLS tokens\n",
    "            idx = idx.sort().values + 1       # shift by +1 to account for CLS at index 0\n",
    "\n",
    "            # Build final list of token indices to keep: [0] (CLS) + selected tokens\n",
    "            keep_idx = torch.cat([\n",
    "                torch.tensor([0], device=x.device, dtype=torch.long),\n",
    "                idx\n",
    "            ])\n",
    "\n",
    "            # Subsample tokens: [B, N+1, C] -> [B, 1+keep, C]\n",
    "            x = x[:, keep_idx]\n",
    "\n",
    "            # 5. Apply attention + MLP on the reduced sequence\n",
    "            x = x + blk.attn(blk.norm1(x))[0]\n",
    "            x = x + blk.mlp(blk.norm2(x))\n",
    "\n",
    "        # Final normalization and classification head on CLS token\n",
    "        x = self.m.norm(x)\n",
    "        return self.m.head(x[:, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3fa0d55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T10:51:09.933831Z",
     "iopub.status.busy": "2025-12-08T10:51:09.933646Z",
     "iopub.status.idle": "2025-12-08T10:51:15.361166Z",
     "shell.execute_reply": "2025-12-08T10:51:15.360306Z"
    },
    "papermill": {
     "duration": 5.431072,
     "end_time": "2025-12-08T10:51:15.362397",
     "exception": false,
     "start_time": "2025-12-08T10:51:09.931325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:02<00:00, 77.5MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 50000 Test samples: 10000\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# CIFAR-10 data loading\n",
    "# -----------------------------\n",
    "\n",
    "# Use local path if available; otherwise fall back to Kaggle working dir\n",
    "data_root = \"data/cifar-10\" if os.path.exists(\"data/cifar-10\") else \"/kaggle/working\"\n",
    "\n",
    "# Basic data augmentation for training, simple resize for test\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.Resize(48),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "test_tf = transforms.Compose([\n",
    "    transforms.Resize(48),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Download + create datasets\n",
    "train_set = datasets.CIFAR10(data_root, train=True,  download=True, transform=train_tf)\n",
    "test_set  = datasets.CIFAR10(data_root, train=False, download=True, transform=test_tf)\n",
    "\n",
    "# Data loaders\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True,  num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_set,  batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(\"Train samples:\", len(train_set), \"Test samples:\", len(test_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b982eba8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T10:51:15.369830Z",
     "iopub.status.busy": "2025-12-08T10:51:15.369176Z",
     "iopub.status.idle": "2025-12-08T10:51:15.770899Z",
     "shell.execute_reply": "2025-12-08T10:51:15.770270Z"
    },
    "papermill": {
     "duration": 0.406683,
     "end_time": "2025-12-08T10:51:15.772222",
     "exception": false,
     "start_time": "2025-12-08T10:51:15.365539",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Model initialization and optimizer\n",
    "# -----------------------------\n",
    "\n",
    "# Base ViT and pruned wrapper\n",
    "base = ViT().to(device)\n",
    "model = SimplePrunedViT(base)\n",
    "\n",
    "# Optionally use DataParallel if multiple GPUs are available\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# AdamW optimizer is standard for transformer models\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=LR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0047bf0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T10:51:15.779114Z",
     "iopub.status.busy": "2025-12-08T10:51:15.778906Z",
     "iopub.status.idle": "2025-12-08T11:38:24.497926Z",
     "shell.execute_reply": "2025-12-08T11:38:24.496841Z"
    },
    "papermill": {
     "duration": 2828.723882,
     "end_time": "2025-12-08T11:38:24.499133",
     "exception": false,
     "start_time": "2025-12-08T10:51:15.775251",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[PRUNED] Epoch 1/10: 100%|██████████| 391/391 [05:05<00:00,  1.28it/s, loss=1.89]\n",
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 summary: time=5.09 min  train_loss=1.8912  val_acc=44.23%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[PRUNED] Epoch 2/10: 100%|██████████| 391/391 [05:18<00:00,  1.23it/s, loss=1.29]\n",
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 summary: time=5.31 min  train_loss=1.2913  val_acc=57.85%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[PRUNED] Epoch 3/10: 100%|██████████| 391/391 [05:19<00:00,  1.22it/s, loss=1.09]\n",
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 summary: time=5.33 min  train_loss=1.0899  val_acc=61.35%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[PRUNED] Epoch 4/10: 100%|██████████| 391/391 [05:20<00:00,  1.22it/s, loss=0.97]\n",
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 summary: time=5.34 min  train_loss=0.9702  val_acc=62.83%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[PRUNED] Epoch 5/10: 100%|██████████| 391/391 [03:43<00:00,  1.75it/s, loss=0.892]\n",
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 summary: time=3.72 min  train_loss=0.8920  val_acc=66.41%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[PRUNED] Epoch 6/10: 100%|██████████| 391/391 [03:43<00:00,  1.75it/s, loss=0.83]\n",
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 summary: time=3.72 min  train_loss=0.8296  val_acc=68.43%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[PRUNED] Epoch 7/10: 100%|██████████| 391/391 [03:43<00:00,  1.75it/s, loss=0.771]\n",
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 summary: time=3.73 min  train_loss=0.7708  val_acc=68.97%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[PRUNED] Epoch 8/10: 100%|██████████| 391/391 [03:43<00:00,  1.75it/s, loss=0.727]\n",
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 summary: time=3.73 min  train_loss=0.7269  val_acc=70.12%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[PRUNED] Epoch 9/10: 100%|██████████| 391/391 [03:43<00:00,  1.75it/s, loss=0.681]\n",
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 summary: time=3.73 min  train_loss=0.6814  val_acc=70.26%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[PRUNED] Epoch 10/10: 100%|██████████| 391/391 [03:43<00:00,  1.75it/s, loss=0.641]\n",
      "                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 summary: time=3.73 min  train_loss=0.6412  val_acc=69.97%\n",
      "\n",
      "Total training time: 43.43 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def format_time(t):\n",
    "    \"\"\"Pretty-print a duration in seconds as either seconds or minutes.\"\"\"\n",
    "    return f\"{t/60:.2f} min\" if t > 60 else f\"{t:.1f} sec\"\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(epoch=None):\n",
    "    \"\"\"Evaluate model accuracy on the test set.\n",
    "\n",
    "    If `epoch` is provided, the model will use the corresponding pruning\n",
    "    behavior (warmup vs pruned) during evaluation as well.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    for x, y in tqdm(test_loader, desc=\"Eval\", leave=False):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model(x, epoch=epoch).argmax(1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Training loop\n",
    "# -----------------------------\n",
    "total_train_time = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    epoch_start = time.time()\n",
    "    running_loss = 0.0\n",
    "    seen = 0\n",
    "\n",
    "    # TQDM progress bar for training\n",
    "    pbar = tqdm(train_loader, desc=f\"[PRUNED] Epoch {epoch}/{EPOCHS}\")\n",
    "\n",
    "    for x, y in pbar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n", 
    "        opt.zero_grad()\n",
    "        # Pass epoch to control pruning schedule inside the model\n",
    "        logits = model(x, epoch=epoch)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # Track average loss\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "        seen += x.size(0)\n",
    "        pbar.set_postfix(loss=running_loss / seen)\n",
    "\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    total_train_time += epoch_time\n",
    "\n",
    "    # --- Validation at the end of each epoch ---\n",
    "    val_acc = evaluate(epoch)\n",
    "\n",
    "    print(\n",
    "        f\"\\nEpoch {epoch} summary:\"\n",
    "        f\" time={format_time(epoch_time)}\"\n",
    "        f\"  train_loss={running_loss/seen:.4f}\"\n",
    "        f\"  val_acc={val_acc*100:.2f}%\\n\"\n",
    "    )\n",
    "\n",
    "print(f\"Total training time: {format_time(total_train_time)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2849.144195,
   "end_time": "2025-12-08T11:38:27.068841",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-08T10:50:57.924646",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
